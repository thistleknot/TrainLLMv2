{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f91de3ed-d5d1-4c62-b54a-f5af04e54bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/distvol/bitsandbytes/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-06-02 09:08:37.625364: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /mnt/distvol/bitsandbytes/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda115_nocublaslt.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 6.1\n",
      "CUDA SETUP: Detected CUDA version 115\n",
      "CUDA SETUP: Loading binary /mnt/distvol/bitsandbytes/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda115_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/distvol/bitsandbytes/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('-I/root')}\n",
      "  warn(msg)\n",
      "/mnt/distvol/bitsandbytes/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('2')}\n",
      "  warn(msg)\n",
      "/mnt/distvol/bitsandbytes/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('-L/root/boost_1_75_0/stage/lib')}\n",
      "  warn(msg)\n",
      "/mnt/distvol/bitsandbytes/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/var/lib/snap')}\n",
      "  warn(msg)\n",
      "/mnt/distvol/bitsandbytes/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('li#41'), PosixPath('k;=\\\\E[21~'), PosixPath('kb=\\x7f'), PosixPath('ks=\\\\E[?1h\\\\E='), PosixPath('FA=\\\\E[19;2~'), PosixPath('po=\\\\E[5i'), PosixPath('nd=\\\\E[C'), PosixPath('DL=\\\\E[%dM'), PosixPath('al=\\\\E[L'), PosixPath('SC|screen|VT 100/ANSI X3.64 virtual terminal'), PosixPath('mr=\\\\E[7m'), PosixPath('co#155'), PosixPath('k2=\\\\EOQ'), PosixPath('F7=\\\\E[15;2~'), PosixPath('DC=\\\\E[%dP'), PosixPath('ti=\\\\E[?1049h'), PosixPath('ei=\\\\E[4l'), PosixPath('te=\\\\E[?1049l'), PosixPath('le=^H'), PosixPath('ac=\\\\140\\\\140aaffggjjkkllmmnnooppqqrrssttuuvvwwxxyyzz{{||}}~~..--++,,hhII00'), PosixPath('dc=\\\\E[P'), PosixPath('dl=\\\\E[M'), PosixPath('\\\\\\n\\t'), PosixPath('me=\\\\E[m'), PosixPath('op=\\\\E[39;49m'), PosixPath('ke=\\\\E[?1l\\\\E>'), PosixPath('AB=\\\\E[4%dm'), PosixPath('F9=\\\\E[18;2~'), PosixPath('AL=\\\\E[%dL'), PosixPath('ct=\\\\E[3g'), PosixPath('LE=\\\\E[%dD'), PosixPath('DO=\\\\E[%dB'), PosixPath('bl=^G'), PosixPath('AF=\\\\E[3%dm'), PosixPath('#4=\\\\E[1;2D'), PosixPath('RI=\\\\E[%dC'), PosixPath('kh=\\\\E[1~'), PosixPath('F2=\\\\E[24~'), PosixPath('kd=\\\\EOB'), PosixPath('rc=\\\\E8'), PosixPath('kB=\\\\E[Z'), PosixPath('%e=\\\\E[5;2~'), PosixPath('FD=\\\\E[23;2~'), PosixPath('IC=\\\\E[%d@'), PosixPath('F5=\\\\E[1;2R'), PosixPath('st=\\\\EH'), PosixPath('@1=\\\\E[1~'), PosixPath('ae=\\\\E(B'), PosixPath('is=\\\\E)0'), PosixPath('ve=\\\\E[34h\\\\E[?25h'), PosixPath('FB=\\\\E[20;2~'), PosixPath('vi=\\\\E[?25l'), PosixPath('rs=\\\\Ec'), PosixPath('cs=\\\\E[%i%d;%dr'), PosixPath('%i=\\\\E[1;2C'), PosixPath('k8=\\\\E[19~'), PosixPath('md=\\\\E[1m'), PosixPath('xn'), PosixPath('K2=\\\\EOE'), PosixPath('mi'), PosixPath('kr=\\\\EOC'), PosixPath('k3=\\\\EOR'), PosixPath('%c=\\\\E[6;2~'), PosixPath('mh=\\\\E[2m'), PosixPath('FC=\\\\E[21;2~'), PosixPath('kD=\\\\E[3~'), PosixPath('F4=\\\\E[1;2Q'), PosixPath('nw=\\\\EE'), PosixPath('F8=\\\\E[17;2~'), PosixPath('kR=\\\\E[1;2A'), PosixPath('im=\\\\E[4h'), PosixPath('so=\\\\E[3m'), PosixPath('do=^J'), PosixPath('ku=\\\\EOA'), PosixPath('k4=\\\\EOS'), PosixPath('up=\\\\EM'), PosixPath('ms'), PosixPath('kP=\\\\E[5~'), PosixPath('k9=\\\\E[20~'), PosixPath('sc=\\\\E7'), PosixPath('cm=\\\\E[%i%d;%dH'), PosixPath('k6=\\\\E[17~'), PosixPath('vs=\\\\E[34l'), PosixPath('*4=\\\\E[3;2~'), PosixPath('G0'), PosixPath('bs'), PosixPath('it#8'), PosixPath('F3=\\\\E[1;2P'), PosixPath('ho=\\\\E[H'), PosixPath('km'), PosixPath('sr=\\\\EM'), PosixPath('cr=^M'), PosixPath('k1=\\\\EOP'), PosixPath('#3=\\\\E[2;2~'), PosixPath('ce=\\\\E[K'), PosixPath('ue=\\\\E[24m'), PosixPath('xv'), PosixPath('vb=\\\\Eg'), PosixPath('FE=\\\\E[24;2~'), PosixPath('UP=\\\\E[%dA'), PosixPath('bt=\\\\E[Z'), PosixPath('Co#8'), PosixPath('ta=^I'), PosixPath('as=\\\\E(0'), PosixPath('pf=\\\\E[4i'), PosixPath('#2=\\\\E[1;2H'), PosixPath('kH=\\\\E[4~'), PosixPath('pa#64'), PosixPath('@7=\\\\E[4~'), PosixPath('F6=\\\\E[1;2S'), PosixPath('Km=\\\\E[M'), PosixPath('pt'), PosixPath('kN=\\\\E[6~'), PosixPath('AX'), PosixPath('kI=\\\\E[2~'), PosixPath('us=\\\\E[4m'), PosixPath('kF=\\\\E[1;2B'), PosixPath('LP'), PosixPath('kl=\\\\EOD'), PosixPath('mb=\\\\E[5m'), PosixPath('k0=\\\\E[10~'), PosixPath('cl=\\\\E[H\\\\E[J'), PosixPath('am'), PosixPath('k7=\\\\E[18~'), PosixPath('F1=\\\\E[23~'), PosixPath('*7=\\\\E[1;2F'), PosixPath('k5=\\\\E[15~'), PosixPath('se=\\\\E[23m'), PosixPath('cd=\\\\E[J')}\n",
      "  warn(msg)\n",
      "/mnt/distvol/bitsandbytes/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  ( alias;\\n eval ${which_declare} ) | /usr/bin/which --tty-only --read-alias --read-functions --show-tilde --show-dot \"$@\"\\n}')}\n",
      "  warn(msg)\n",
      "/mnt/distvol/bitsandbytes/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('-}\" ]; then\\n eval `eval ${_mlre} /usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash \\'\"$@\"\\'`;\\n else\\n eval `/usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash \"$@\"`;\\n fi;\\n _mlstatus=$?;\\n if [ -n \"${_mlIFS+x}\" ]; then\\n IFS=$_mlIFS;\\n else\\n unset IFS;\\n fi;\\n unset _mlre _mlv _mlrv _mlIFS;\\n if [ -n \"${_mlshdbg'), PosixPath(\"-}${_mlv}='`eval 'echo ${'$_mlrv'\"), PosixPath('-}\" ]; then\\n set -$_mlshdbg;\\n fi;\\n unset _mlshdbg;\\n return $_mlstatus\\n}'), PosixPath('-};\\n do\\n if [ \"${_mlv}\" = \"${_mlv##*[!A-Za-z0-9_]}\" -a \"${_mlv}\" = \"${_mlv#[0-9]}\" ]; then\\n if [ -n \"`eval \\'echo ${\\'$_mlv\\'+x}\\'`\" ]; then\\n _mlre=\"${_mlre'), PosixPath('-}${_mlv}_modquar=\\'`eval \\'echo ${\\'$_mlv\\'}\\'`\\' \";\\n fi;\\n _mlrv=\"MODULES_RUNENV_${_mlv}\";\\n _mlre=\"${_mlre'), PosixPath('() {  unset _mlshdbg;\\n if [ \"${MODULES_SILENT_SHELL_DEBUG'), PosixPath('-0}\" = \\'1\\' ]; then\\n case \"$-\" in \\n *v*x*)\\n set +vx;\\n _mlshdbg=\\'vx\\'\\n ;;\\n *v*)\\n set +v;\\n _mlshdbg=\\'v\\'\\n ;;\\n *x*)\\n set +x;\\n _mlshdbg=\\'x\\'\\n ;;\\n *)\\n _mlshdbg=\\'\\'\\n ;;\\n esac;\\n fi;\\n unset _mlre _mlIFS;\\n if [ -n \"${IFS+x}\" ]; then\\n _mlIFS=$IFS;\\n fi;\\n IFS=\\' \\';\\n for _mlv in ${MODULES_RUN_QUARANTINE'), PosixPath('-}\\'`\\' \";\\n fi;\\n done;\\n if [ -n \"${_mlre')}\n",
      "  warn(msg)\n",
      "/mnt/distvol/bitsandbytes/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  typeset swfound=1;\\n if [ \"${MODULES_USE_COMPAT_VERSION'), PosixPath('-0}\" = \\'1\\' ]; then\\n typeset swname=\\'main\\';\\n if [ -e /usr/share/Modules/libexec/modulecmd.tcl ]; then\\n typeset swfound=0;\\n unset MODULES_USE_COMPAT_VERSION;\\n fi;\\n else\\n typeset swname=\\'compatibility\\';\\n if [ -e /usr/share/Modules/libexec/modulecmd-compat ]; then\\n typeset swfound=0;\\n MODULES_USE_COMPAT_VERSION=1;\\n export MODULES_USE_COMPAT_VERSION;\\n fi;\\n fi;\\n if [ $swfound -eq 0 ]; then\\n echo \"Switching to Modules $swname version\";\\n source /usr/share/Modules/init/bash;\\n else\\n echo \"Cannot switch to Modules $swname version, command not found\";\\n return 1;\\n fi\\n}')}\n",
      "  warn(msg)\n",
      "/mnt/distvol/bitsandbytes/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/mnt/distvol/bitsandbytes/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "/mnt/distvol/bitsandbytes/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfrom peft import prepare_model_for_kbit_training\\n\\nfrom transformers import TrainerCallback\\nfrom torch.cuda.amp import autocast\\nfrom torch.optim import AdamW\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, Trainer, EvalPrediction, TrainingArguments, TrainerControl, TrainerState, BitModel\n",
    "import math\n",
    "#from transformers.trainer_pt_utils import PredictionOutput\n",
    "from peft import LoraConfig, get_peft_model, LoraModel\n",
    "from peft import prepare_model_for_kbit_training, PeftModel, PeftConfig\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset#, Dataset\n",
    "import datasets\n",
    "import numpy as np\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "from typing import List, Optional  # Add the import statement at the beginning of your file\n",
    "from transformers import logging\n",
    "from typing import Dict, Optional, Any\n",
    "from tqdm import tqdm\n",
    "from transformers import TrainerState\n",
    "from datetime import datetime\n",
    "import copy\n",
    "from transformers import TrainerControl, TrainerState\n",
    "import tempfile\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import pickle\n",
    "from random import sample\n",
    "\"\"\"\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "from transformers import TrainerCallback\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.optim import AdamW\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "517260a2-a489-4118-b7cc-f98b35fa8454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subset(dataset, num_examples):\n",
    "    indices = sample(range(len(dataset)), num_examples)\n",
    "    return dataset.select(indices)\n",
    "\n",
    "def filter_datasets_for_use_case(datasets, use_case):\n",
    "    filtered_datasets = {}\n",
    "    for key, value in datasets.items():\n",
    "        if value[use_case]:\n",
    "            filtered_datasets[key] = value[use_case]\n",
    "    return filtered_datasets\n",
    "\n",
    "def split_datasets(data_dict, ratio=0.7, random_state=None):\n",
    "    train_data = {}\n",
    "    test_data = {}\n",
    "    validation_indices = {}\n",
    "\n",
    "    for key, value in data_dict.items():\n",
    "        train, test, train_indices, test_indices = train_test_split(value, range(len(value)), train_size=ratio, random_state=random_state)\n",
    "        train_data[key] = train\n",
    "        test_data[key] = test\n",
    "        validation_indices[key] = test_indices\n",
    "\n",
    "    return train_data, test_data, validation_indices\n",
    "\n",
    "def unique_elements(lst):\n",
    "    result = []\n",
    "    seen = set()\n",
    "    for item in lst:\n",
    "        if item not in seen:\n",
    "            seen.add(item)\n",
    "            result.append(item)\n",
    "    return result\n",
    "\n",
    "class PerplexityLoggingCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args: TrainingArguments, state: TrainerState, control: TrainerControl,\n",
    "                    metrics: Dict[str, float], prefix=None, **kwargs):\n",
    "        if prefix is None:\n",
    "            prefix = \"eval\"\n",
    "        eval_loss_key = f\"{prefix}_loss\"\n",
    "        if eval_loss_key in metrics:\n",
    "            loss = metrics[eval_loss_key]\n",
    "            metrics[f\"{prefix}_perplexity\"] = math.exp(loss)\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tensor_list):\n",
    "        self.tensor_list = tensor_list\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tensor_list[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensor_list)\n",
    "        \n",
    "def get_sequences(text, tokenizer, seq_length=768, stride_ratio=0.5):\n",
    "    all_token_ids = tokenizer.encode(text)\n",
    "\n",
    "    #Generate sequences using sliding window approach\n",
    "    stride_length = int(seq_length * stride_ratio)\n",
    "    sequences = []\n",
    "    for i in range(0, len(all_token_ids) - seq_length +1, stride_length):\n",
    "        input_ids = all_token_ids[i:i+seq_length]\n",
    "        sequences.append(input_ids)\n",
    "    \n",
    "    #Truncate the last sequence if it less than seq_length\n",
    "    last_sequence = sequences[-1]\n",
    "    if len(last_sequence) < seq_length:\n",
    "        last_sequence = last_sequence + [tokenizer.pad_token_id] * (seq_length - len(last_sequence))\n",
    "        sequences[-1] = last_sequence\n",
    "\n",
    "    #Drop any remaining sequences that are less than seq_length\n",
    "    sequences = [sequence for sequence in sequences if len(sequence) == seq_length]\n",
    "\n",
    "    return sequences\n",
    "\n",
    "def evaluate(model, dataloader, device, max_eval_steps):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        # Extract input_ids and convert them to tensors\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device) if 'labels' in batch else None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            input_dict = {'input_ids': input_ids, 'labels': labels}\n",
    "            outputs = model(**input_dict)\n",
    "         \n",
    "        loss = outputs.loss.repeat(input_ids.shape[0])\n",
    "        losses.append(loss.detach())\n",
    "        if max_eval_steps > 0 and step >= max_eval_steps: break\n",
    "    loss = torch.mean(torch.cat(losses))\n",
    "    try:\n",
    "        perplexity = torch.exp(loss)\n",
    "    except OverflowError:\n",
    "        perplexity = torch.tensor(float(\"inf\"))\n",
    "    return loss.item(), perplexity.item()\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, max_eval_steps=0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.best_perplexity = float(\"inf\")\n",
    "        self.best_model_state_dict = None\n",
    "        self.no_improvement_counter = 0\n",
    "        self.passed_epoch_steps = False\n",
    "        self.max_eval_steps = max_eval_steps  # Add max_eval_steps as an attribute\n",
    "\n",
    "    def evaluation_loop(self, dataloader, description, prediction_loss_only=False, ignore_keys=None, metric_key_prefix='eval'):\n",
    "        eval_loss, perplexity = evaluate(self.model, dataloader, self.args.device, self.max_eval_steps)\n",
    "    \n",
    "        # Check if epoch_steps are surpassed\n",
    "        if self.state.epoch >= 1:\n",
    "            self.passed_epoch_steps = True\n",
    "    \n",
    "        # Check for improvements if the epoch_steps are surpassed\n",
    "        if self.passed_epoch_steps:\n",
    "            if perplexity < self.best_perplexity:\n",
    "                self.best_perplexity = perplexity\n",
    "                self.best_model_state_dict = {k: v.clone().to('cpu') for k, v in self.model.state_dict().items()}\n",
    "                self.no_improvement_counter = 0\n",
    "            else:\n",
    "                self.no_improvement_counter += 1\n",
    "    \n",
    "        # Stop training, load the best state_dict in the model, and return the best_model if the perplexity did not improve 3 times consecutively\n",
    "        if self.no_improvement_counter == 3:\n",
    "            if self.best_model_state_dict:\n",
    "                self.model.load_state_dict(self.best_model_state_dict)\n",
    "            self.model.to(self.args.device)\n",
    "            self.control.should_training_stop = True\n",
    "            print(\"Training stopped, best model loaded with Perplexity:\", self.best_perplexity)\n",
    "    \n",
    "        self.log({\n",
    "            \"eval_loss\": eval_loss,\n",
    "            \"perplexity\": perplexity,\n",
    "            \"epoch\": self.state.epoch,\n",
    "        })\n",
    "    \n",
    "        # Define num_samples as the total number of samples in the dataloader\n",
    "        #num_samples = len(dataloader.dataset)\n",
    "    \n",
    "        # Initialize an instance of EvalPrediction without the 'metrics' keyword argument \n",
    "        #eval_prediction = EvalPrediction(predictions=None, label_ids=None, num_samples=num_samples)\n",
    "        eval_prediction = EvalPrediction(predictions=None, label_ids=None)\n",
    "        \n",
    "        # Define num_samples as the total number of samples in the dataloader\n",
    "        num_samples = len(dataloader.dataset)\n",
    "    \n",
    "        # Add the num_samples attribute to the eval_prediction instance\n",
    "        eval_prediction.num_samples = num_samples\n",
    "    \n",
    "        # Set the metrics dictionary\n",
    "        eval_prediction.metrics = {\"eval_loss\": eval_loss}\n",
    "    \n",
    "        return eval_prediction\n",
    "    \n",
    "    def get_completed_steps(self):\n",
    "        return self.state.global_step\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ed5da9-cff8-4ae8-b27f-16df0a16cb81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb5966da-98d0-4321-bff0-923509d368cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294912 || all params: 83026176 || trainable%: 0.3552036408373186\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#sample # of character's from combined_text\n",
    "sample=False\n",
    "#if true, what # of characters to sample (this * avg_tokens_per_char = rough # of tokens)\n",
    "#(10000*.6)/2/128*.1\n",
    "s_size = 10000\n",
    "seq_length = 128\n",
    "#seq_length = 128\n",
    "batch_size = 16\n",
    "epoch_steps_warmup_ratio = 1/3\n",
    "epochs = 10\n",
    "model_id = \"EleutherAI/gpt-neo-1.3B\"\n",
    "#model_id = \"EleutherAI/gpt-neo-125M\"\n",
    "warm_ratio = 1/2\n",
    "train_fraction = 0.9\n",
    "epochs = 3\n",
    "gradient_accumulation_steps = 16\n",
    "seed = 42\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    #bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_quant_type=\"fp4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8, \n",
    "    lora_alpha=32, \n",
    "    #target_modules=[\"query_key_value\"], \n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "#with open(\"bits/bnb_config.json\", \"r\") as f:\n",
    "    #bnb_config = json.load(f)\n",
    "\n",
    "#lora_config = lora_config.from_pretrained('bits')\n",
    "\n",
    "#model_id = '/root/.cache/huggingface/hub/models--EleutherAI--gpt-neo-1.3B/snapshots/0f35a20339a9e208bc061570981180cfb87c9572'\n",
    "\n",
    "\n",
    "\n",
    "peft_config = PeftConfig.from_pretrained('bits')\n",
    "#BitModel.from_pretrained(peft_config)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        peft_config.base_model_name_or_path,\n",
    "        quantization_config=bnb_config, device_map={\"\":0}\n",
    "        #load_in_8bit=False,\n",
    "        #return_dict=True,\n",
    "        #device_map=\"auto\",\n",
    "        #torch_dtype=torch.float16,\n",
    "        #low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print_trainable_parameters(model)\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b886fe30-ffbe-4c7d-b7ce-7b0feacdfbf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPTNeoForCausalLM(\n",
       "      (transformer): GPTNeoModel(\n",
       "        (wte): Embedding(50257, 768)\n",
       "        (wpe): Embedding(2048, 768)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-11): 12 x GPTNeoBlock(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTNeoAttention(\n",
       "              (attention): GPTNeoSelfAttention(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (k_proj): Linear4bit(in_features=768, out_features=768, bias=False)\n",
       "                (v_proj): Linear4bit(\n",
       "                  in_features=768, out_features=768, bias=False\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (q_proj): Linear4bit(\n",
       "                  in_features=768, out_features=768, bias=False\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (out_proj): Linear4bit(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPTNeoMLP(\n",
       "              (c_fc): Linear4bit(in_features=768, out_features=3072, bias=True)\n",
       "              (c_proj): Linear4bit(in_features=3072, out_features=768, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17868161-7357-4322-87a1-5c8aa4210b3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f431abf-5d40-400e-84b7-1bcc879e20bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8810418f-d812-459c-a828-34dbd45525f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76a6e5c-6c81-4d01-a686-976595067a35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8543e31a-d0ee-40b7-b264-9f163e375aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0 || all params: 83026176 || trainable%: 0.0\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "lora_config = LoraConfig.from_pretrained('bits')\n",
    "peft_config = PeftConfig.from_pretrained('bits')\n",
    "model = AutoModelForCausalLM.from_pretrained(peft_config.base_model_name_or_path,quantization_config=bnb_config, device_map={\"\":0})\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print_trainable_parameters(model)\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66980623-06de-4529-8fce-7278f210f508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439c28ce-b8a7-4de6-b572-a077db12a680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb6ab63-98c1-42b2-86f8-d2b3a6cf5778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6642d92-6546-4bf2-a536-09ccecdb76f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440c7f44-9a1e-4bab-92da-10d0dcedfe7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76f48c43-201b-40e8-bf9a-a7f5c1747673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before load\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (80368 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"before load\")\n",
    "with open('../venv_train_neo/datasets_dict.pkl', 'rb') as f:\n",
    "    datasets_dict = pickle.load(f)\n",
    "    \n",
    "finetune_datasets = filter_datasets_for_use_case(datasets_dict, 'finetune')\n",
    "\n",
    "train_data_list, valid_data_list, valid_data_indices = split_datasets(finetune_datasets, ratio=0.7, random_state=seed)\n",
    "\n",
    "train_data_list = [record for dataset in train_data_list.values() for record in dataset]\n",
    "valid_data_list = [record for dataset in valid_data_list.values() for record in dataset]\n",
    "\n",
    "# Replace existing strings with special tokens\n",
    "train_data_list = [\n",
    "    example.replace(\"Context: \", \"<|Context|>\")\n",
    "            .replace(\"Prompt: \", \"<|Prompt|>\")\n",
    "            .replace(\"Response: \", \"<|Response|>\") \n",
    "    for example in train_data_list\n",
    "]\n",
    "\"\"\"\n",
    "# Replace existing strings with special tokens\n",
    "valid_data_list = [\n",
    "    example.replace(\"Context: \", \"<|Context|>\")\n",
    "            .replace(\"Prompt: \", \"<|Prompt|>\")\n",
    "            .replace(\"Response: \", \"<|Response|>\") \n",
    "    for example in valid_data_list\n",
    "]\n",
    "\n",
    "new_tokens = {'additional_special_tokens': ['<|Context|>', '<|Prompt|>', '<|Response|>']}\n",
    "tokenizer.add_special_tokens(new_tokens)\n",
    "\"\"\"\n",
    "tokenizer.pad_token = '[PAD]'\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "combined_train = tokenizer.eos_token.join(train_data_list)\n",
    "combined_valid = tokenizer.eos_token.join(valid_data_list)\n",
    "\n",
    "train_sequences = get_sequences(combined_train, tokenizer, seq_length=seq_length)\n",
    "valid_sequences = get_sequences(combined_valid, tokenizer, seq_length=seq_length)\n",
    "\n",
    "train_epoch_steps = (len(train_sequences) / (batch_size * gradient_accumulation_steps))\n",
    "valid_epoch_steps = (len(valid_sequences) / (batch_size * gradient_accumulation_steps))\n",
    "\n",
    "max_train_steps = int(train_epoch_steps * epochs)\n",
    "\n",
    "train_dataset = datasets.Dataset.from_dict({\"input_ids\": train_sequences})\n",
    "valid_dataset = datasets.Dataset.from_dict({\"input_ids\": valid_sequences})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf2472c-4ad1-446c-b354-29f322e9e4ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa379328-26f2-4764-80ee-39d31b706369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23e7c896-94a6-4c85-88ff-874794b0a4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "# Replace 'valid_dataset' with your current evaluation dataset variable\n",
    "num_eval_examples = 16  # Set the number of examples you want to use for evaluation\n",
    "subset_valid_dataset = create_subset(valid_dataset, num_eval_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b824cc2-b148-4ab9-b605-9e4374369748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/mnt/distvol/bitsandbytes/lib/python3.9/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:197: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:493.)\n",
      "  attn_weights = torch.where(causal_mask, attn_weights, mask_value)\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#trainer = Trainer(\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=subset_valid_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size = batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        warmup_steps=int(train_epoch_steps * warm_ratio),\n",
    "        evaluation_strategy='steps',\n",
    "        max_steps=max_train_steps,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.1,      # Add weight decay argument\n",
    "        lr_scheduler_type=\"cosine\",     # Add lr scheduler type argument\n",
    "        max_grad_norm=1.0,       # Add max norm clipping argument\n",
    "        fp16=True,  # Add a keyword here\n",
    "        logging_steps=int(np.clip(np.round(train_epoch_steps/10),1,1)),\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\"\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "    callbacks=[PerplexityLoggingCallback()],  # Add the custom callback\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8531a825-2070-4302-a9a4-f0edc2f96d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_completed_steps = trainer.get_completed_steps()\n",
    "\n",
    "valid_steps = max(1, int(np.round((initial_completed_steps/train_epoch_steps * valid_epoch_steps),0)))\n",
    "\n",
    "#valid_steps = int(np.clip(np.round(initial_completed_steps/train_epoch_steps*valid_epoch_steps,0),1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba8994b-bad2-4a93-bb3d-39729cdc2caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "valid_trainer = CustomTrainer(\n",
    "    model=trainer.model,\n",
    "    max_eval_steps=valid_steps,  # Pass the valid_steps here\n",
    "    train_dataset=valid_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size = batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        max_steps=valid_steps,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.1,      # Add weight decay argument\n",
    "        lr_scheduler_type=\"cosine\",     # Add lr scheduler type argument\n",
    "        max_grad_norm=1.0,      # Add max norm clipping argument\n",
    "        fp16=True,\n",
    "        logging_steps=int(np.clip(np.round(valid_epoch_steps/10),1,1)),\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\"\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "valid_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52e3704-ff87-4773-8457-071b09419695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d98835-d689-4522-9f46-1b3bacd1e99c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b647ece0-7b47-4adb-87ea-7ee9a32f71ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_trainer.model.save_pretrained('./bitsft')\n",
    "\n",
    "\n",
    "valid_trainer.model.gradient_checkpointing_enable()\n",
    "\n",
    "valid_trainer.model.config.use_cache = True\n",
    "\n",
    "generator = pipeline('text-generation', model=valid_trainer.model, tokenizer = tokenizer)\n",
    "#results = generator(r\"Context:\\nPrompt: Finish the quote: 'To live well...'\\nResponse:\", do_sample=True, min_length=50, max_length=200)\n",
    "results = generator(r\"Context:\\nPrompt: If value A is 2, and value B is 3, what is A+B?'\\nResponse:\", do_sample=True, min_length=50, max_length=200)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1b32d1-8da4-4005-92fd-e92fead82afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
