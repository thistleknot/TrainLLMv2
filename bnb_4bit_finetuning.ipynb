{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f91de3ed-d5d1-4c62-b54a-f5af04e54bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom peft import prepare_model_for_kbit_training\\n\\nfrom transformers import TrainerCallback\\nfrom torch.cuda.amp import autocast\\nfrom torch.optim import AdamW\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, Trainer, EvalPrediction, TrainingArguments, TrainerControl, TrainerState\n",
    "import math\n",
    "#from transformers.trainer_pt_utils import PredictionOutput\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from peft import prepare_model_for_kbit_training\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset#, Dataset\n",
    "import datasets\n",
    "import numpy as np\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "from typing import List, Optional  # Add the import statement at the beginning of your file\n",
    "from transformers import logging\n",
    "from typing import Dict, Optional, Any\n",
    "from tqdm import tqdm\n",
    "from transformers import TrainerState\n",
    "from datetime import datetime\n",
    "import copy\n",
    "from transformers import TrainerControl, TrainerState\n",
    "import tempfile\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import pickle\n",
    "\"\"\"\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "from transformers import TrainerCallback\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.optim import AdamW\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "517260a2-a489-4118-b7cc-f98b35fa8454",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_datasets_for_use_case(datasets, use_case):\n",
    "    filtered_datasets = {}\n",
    "    for key, value in datasets.items():\n",
    "        if value[use_case]:\n",
    "            filtered_datasets[key] = value[use_case]\n",
    "    return filtered_datasets\n",
    "\n",
    "def split_datasets(data_dict, ratio=0.7, random_state=None):\n",
    "    train_data = {}\n",
    "    test_data = {}\n",
    "    validation_indices = {}\n",
    "\n",
    "    for key, value in data_dict.items():\n",
    "        train, test, train_indices, test_indices = train_test_split(value, range(len(value)), train_size=ratio, random_state=random_state)\n",
    "        train_data[key] = train\n",
    "        test_data[key] = test\n",
    "        validation_indices[key] = test_indices\n",
    "\n",
    "    return train_data, test_data, validation_indices\n",
    "\n",
    "def unique_elements(lst):\n",
    "    result = []\n",
    "    seen = set()\n",
    "    for item in lst:\n",
    "        if item not in seen:\n",
    "            seen.add(item)\n",
    "            result.append(item)\n",
    "    return result\n",
    "\n",
    "class PerplexityLoggingCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args: TrainingArguments, state: TrainerState, control: TrainerControl,\n",
    "                    metrics: Dict[str, float], prefix=None, **kwargs):\n",
    "        if prefix is None:\n",
    "            prefix = \"eval\"\n",
    "        eval_loss_key = f\"{prefix}_loss\"\n",
    "        if eval_loss_key in metrics:\n",
    "            loss = metrics[eval_loss_key]\n",
    "            metrics[f\"{prefix}_perplexity\"] = math.exp(loss)\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tensor_list):\n",
    "        self.tensor_list = tensor_list\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tensor_list[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensor_list)\n",
    "        \n",
    "def get_sequences(text, tokenizer, seq_length=768, stride_ratio=0.5):\n",
    "    all_token_ids = tokenizer.encode(text)\n",
    "\n",
    "    #Generate sequences using sliding window approach\n",
    "    stride_length = int(seq_length * stride_ratio)\n",
    "    sequences = []\n",
    "    for i in range(0, len(all_token_ids) - seq_length +1, stride_length):\n",
    "        input_ids = all_token_ids[i:i+seq_length]\n",
    "        sequences.append(input_ids)\n",
    "    \n",
    "    #Truncate the last sequence if it less than seq_length\n",
    "    last_sequence = sequences[-1]\n",
    "    if len(last_sequence) < seq_length:\n",
    "        last_sequence = last_sequence + [tokenizer.pad_token_id] * (seq_length - len(last_sequence))\n",
    "        sequences[-1] = last_sequence\n",
    "\n",
    "    #Drop any remaining sequences that are less than seq_length\n",
    "    sequences = [sequence for sequence in sequences if len(sequence) == seq_length]\n",
    "\n",
    "    return sequences\n",
    "\n",
    "def evaluate(model, dataloader, device, max_eval_steps):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        # Extract input_ids and convert them to tensors\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device) if 'labels' in batch else None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            input_dict = {'input_ids': input_ids, 'labels': labels}\n",
    "            outputs = model(**input_dict)\n",
    "         \n",
    "        loss = outputs.loss.repeat(input_ids.shape[0])\n",
    "        losses.append(loss.detach())\n",
    "        if max_eval_steps > 0 and step >= max_eval_steps: break\n",
    "    loss = torch.mean(torch.cat(losses))\n",
    "    try:\n",
    "        perplexity = torch.exp(loss)\n",
    "    except OverflowError:\n",
    "        perplexity = torch.tensor(float(\"inf\"))\n",
    "    return loss.item(), perplexity.item()\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, max_eval_steps=0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.best_perplexity = float(\"inf\")\n",
    "        self.best_model_state_dict = None\n",
    "        self.no_improvement_counter = 0\n",
    "        self.passed_epoch_steps = False\n",
    "        self.max_eval_steps = max_eval_steps  # Add max_eval_steps as an attribute\n",
    "\n",
    "    def evaluation_loop(self, dataloader, description, prediction_loss_only=False, ignore_keys=None, metric_key_prefix='eval'):\n",
    "        eval_loss, perplexity = evaluate(self.model, dataloader, self.args.device, self.max_eval_steps)\n",
    "    \n",
    "        # Check if epoch_steps are surpassed\n",
    "        if self.state.epoch >= 1:\n",
    "            self.passed_epoch_steps = True\n",
    "    \n",
    "        # Check for improvements if the epoch_steps are surpassed\n",
    "        if self.passed_epoch_steps:\n",
    "            if perplexity < self.best_perplexity:\n",
    "                self.best_perplexity = perplexity\n",
    "                self.best_model_state_dict = {k: v.clone().to('cpu') for k, v in self.model.state_dict().items()}\n",
    "                self.no_improvement_counter = 0\n",
    "            else:\n",
    "                self.no_improvement_counter += 1\n",
    "    \n",
    "        # Stop training, load the best state_dict in the model, and return the best_model if the perplexity did not improve 3 times consecutively\n",
    "        if self.no_improvement_counter == 3:\n",
    "            if self.best_model_state_dict:\n",
    "                self.model.load_state_dict(self.best_model_state_dict)\n",
    "            self.model.to(self.args.device)\n",
    "            self.control.should_training_stop = True\n",
    "            print(\"Training stopped, best model loaded with Perplexity:\", self.best_perplexity)\n",
    "    \n",
    "        self.log({\n",
    "            \"eval_loss\": eval_loss,\n",
    "            \"perplexity\": perplexity,\n",
    "            \"epoch\": self.state.epoch,\n",
    "        })\n",
    "    \n",
    "        # Define num_samples as the total number of samples in the dataloader\n",
    "        #num_samples = len(dataloader.dataset)\n",
    "    \n",
    "        # Initialize an instance of EvalPrediction without the 'metrics' keyword argument \n",
    "        #eval_prediction = EvalPrediction(predictions=None, label_ids=None, num_samples=num_samples)\n",
    "        eval_prediction = EvalPrediction(predictions=None, label_ids=None)\n",
    "        \n",
    "        # Define num_samples as the total number of samples in the dataloader\n",
    "        num_samples = len(dataloader.dataset)\n",
    "    \n",
    "        # Add the num_samples attribute to the eval_prediction instance\n",
    "        eval_prediction.num_samples = num_samples\n",
    "    \n",
    "        # Set the metrics dictionary\n",
    "        eval_prediction.metrics = {\"eval_loss\": eval_loss}\n",
    "    \n",
    "        return eval_prediction\n",
    "    \n",
    "    def get_completed_steps(self):\n",
    "        return self.state.global_step\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb5966da-98d0-4321-bff0-923509d368cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#sample # of character's from combined_text\n",
    "sample=False\n",
    "#if true, what # of characters to sample (this * avg_tokens_per_char = rough # of tokens)\n",
    "#(10000*.6)/2/128*.1\n",
    "s_size = 10000\n",
    "seq_length = 128\n",
    "#seq_length = 128\n",
    "batch_size = 1\n",
    "epoch_steps_warmup_ratio = 1/3\n",
    "epochs = 10\n",
    "model_id = \"EleutherAI/gpt-neo-1.3B\"\n",
    "#model_id = \"EleutherAI/gpt-neo-125M\"\n",
    "warm_ratio = 1/2\n",
    "train_fraction = 0.9\n",
    "epochs = 3\n",
    "gradient_accumulation_steps = 16\n",
    "seed = 42\n",
    "\n",
    "#model_id = \"openlm-research/open_llama_3b_600bt_preview\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    #bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_quant_type=\"fp4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8, \n",
    "    lora_alpha=32, \n",
    "    #target_modules=[\"query_key_value\"], \n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\n",
    "lora_config = LoraConfig.from_pretrained('bits-25-1.3')\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440c7f44-9a1e-4bab-92da-10d0dcedfe7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76f48c43-201b-40e8-bf9a-a7f5c1747673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before load\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (112676 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(all_sequences) 1759\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([128])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(\"before load\")\n",
    "with open('../venv_train_neo/datasets_dict.pkl', 'rb') as f:\n",
    "    datasets_dict = pickle.load(f)\n",
    "    \n",
    "finetune_datasets = filter_datasets_for_use_case(datasets_dict, 'finetune')\n",
    "train_data_list, valid_data_list, valid_data_indices = split_datasets(finetune_datasets, ratio=0.7, random_state=seed)\n",
    "\n",
    "train_data_list = [record for dataset in train_data_list.values() for record in dataset]\n",
    "valid_data_list = [record for dataset in valid_data_list.values() for record in dataset]\n",
    "\n",
    "dataset = [*train_data_list,*valid_data_list]\n",
    "\n",
    "combined_text = tokenizer.eos_token.join(dataset)\n",
    "\n",
    "if(sample):\n",
    "    combined_text = combined_text[0:s_size]\n",
    "else:\n",
    "    pass\n",
    "\n",
    "all_sequences = get_sequences(combined_text, tokenizer, seq_length=seq_length)\n",
    "print(\"len(all_sequences)\",len(all_sequences))\n",
    "\n",
    "total_sequences = len(all_sequences)\n",
    "train_epoch_steps  = (total_sequences / (batch_size * gradient_accumulation_steps))*train_fraction\n",
    "test_epoch_steps  = (total_sequences / (batch_size * gradient_accumulation_steps))*(1-train_fraction)\n",
    "\n",
    "max_train_steps = int(train_epoch_steps * epochs)\n",
    "\n",
    "train_len = int(train_fraction * total_sequences) \n",
    "\n",
    "train_sequences = all_sequences[:train_len]\n",
    "test_sequences = all_sequences[train_len:]\n",
    "\n",
    "#train_dataset = CustomDataset(train_sequences)\n",
    "#test_dataset = CustomDataset(test_sequences)\n",
    "\n",
    "train_dataset = datasets.Dataset.from_dict({\"input_ids\": train_sequences})\n",
    "test_dataset = datasets.Dataset.from_dict({\"input_ids\": test_sequences})\n",
    "\n",
    "#len(train_dataset[0]['input_ids'])\n",
    "\n",
    "np.unique([len(m) for m in train_sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbf2472c-4ad1-446c-b354-29f322e9e4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = '[PAD]'\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b824cc2-b148-4ab9-b605-9e4374369748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">23</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">20 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>callbacks=[PerplexityLoggingCallback()],  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Add the custom callback</span>                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">21 </span>)                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">22 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>23 trainer.train()                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">24 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/mnt/distvol/bitsandbytes/lib/python3.9/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1696</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train</span>      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1693 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>inner_training_loop = find_executable_batch_size(                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1694 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._inner_training_loop, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._train_batch_size, args.auto_find_batch_size  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1695 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1696 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> inner_training_loop(                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1697 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>args=args,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1698 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>resume_from_checkpoint=resume_from_checkpoint,                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1699 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>trial=trial,                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/mnt/distvol/bitsandbytes/lib/python3.9/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2035</span> in            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_inner_training_loop</span>                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2032 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   │   </span>xm.optimizer_step(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.optimizer)                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2033 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.do_grad_scaling:                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2034 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span>scale_before = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.scaler.get_scale()                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2035 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.scaler.step(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.optimizer)                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2036 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.scaler.update()                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2037 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span>scale_after = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.scaler.get_scale()                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2038 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span>optimizer_was_run = scale_before &lt;= scale_after                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/mnt/distvol/bitsandbytes/lib/python3.9/site-packages/torch/cuda/amp/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">grad_scaler.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">339</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">step</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">336 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> optimizer_state[<span style=\"color: #808000; text-decoration-color: #808000\">\"stage\"</span>] <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> OptState.READY:                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">337 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.unscale_(optimizer)                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">338 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>339 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">assert</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(optimizer_state[<span style=\"color: #808000; text-decoration-color: #808000\">\"found_inf_per_device\"</span>]) &gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">\"No inf checks were rec</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">340 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">341 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>retval = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">342 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">AssertionError: </span>No inf checks were recorded for this optimizer.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m23\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m20 \u001b[0m\u001b[2m│   \u001b[0mcallbacks=[PerplexityLoggingCallback()],  \u001b[2m# Add the custom callback\u001b[0m                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m21 \u001b[0m)                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m22 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m23 trainer.train()                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m24 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/mnt/distvol/bitsandbytes/lib/python3.9/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1696\u001b[0m in \u001b[92mtrain\u001b[0m      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1693 \u001b[0m\u001b[2m│   │   \u001b[0minner_training_loop = find_executable_batch_size(                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1694 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._inner_training_loop, \u001b[96mself\u001b[0m._train_batch_size, args.auto_find_batch_size  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1695 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1696 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m inner_training_loop(                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1697 \u001b[0m\u001b[2m│   │   │   \u001b[0margs=args,                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1698 \u001b[0m\u001b[2m│   │   │   \u001b[0mresume_from_checkpoint=resume_from_checkpoint,                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1699 \u001b[0m\u001b[2m│   │   │   \u001b[0mtrial=trial,                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/mnt/distvol/bitsandbytes/lib/python3.9/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m2035\u001b[0m in            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_inner_training_loop\u001b[0m                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2032 \u001b[0m\u001b[2m│   │   │   │   │   │   │   \u001b[0mxm.optimizer_step(\u001b[96mself\u001b[0m.optimizer)                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2033 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94melif\u001b[0m \u001b[96mself\u001b[0m.do_grad_scaling:                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2034 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mscale_before = \u001b[96mself\u001b[0m.scaler.get_scale()                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2035 \u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.scaler.step(\u001b[96mself\u001b[0m.optimizer)                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2036 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.scaler.update()                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2037 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mscale_after = \u001b[96mself\u001b[0m.scaler.get_scale()                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2038 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0moptimizer_was_run = scale_before <= scale_after                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/mnt/distvol/bitsandbytes/lib/python3.9/site-packages/torch/cuda/amp/\u001b[0m\u001b[1;33mgrad_scaler.py\u001b[0m:\u001b[94m339\u001b[0m in \u001b[92mstep\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m336 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m optimizer_state[\u001b[33m\"\u001b[0m\u001b[33mstage\u001b[0m\u001b[33m\"\u001b[0m] \u001b[95mis\u001b[0m OptState.READY:                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m337 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.unscale_(optimizer)                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m338 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m339 \u001b[2m│   │   \u001b[0m\u001b[94massert\u001b[0m \u001b[96mlen\u001b[0m(optimizer_state[\u001b[33m\"\u001b[0m\u001b[33mfound_inf_per_device\u001b[0m\u001b[33m\"\u001b[0m]) > \u001b[94m0\u001b[0m, \u001b[33m\"\u001b[0m\u001b[33mNo inf checks were rec\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m340 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m341 \u001b[0m\u001b[2m│   │   \u001b[0mretval = \u001b[96mself\u001b[0m._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m342 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mAssertionError: \u001b[0mNo inf checks were recorded for this optimizer.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#trainer = Trainer(\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size = batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        warmup_steps=int(train_epoch_steps * warm_ratio),\n",
    "        evaluation_strategy='steps',\n",
    "        max_steps=max_train_steps,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,  # Add a keyword here\n",
    "        logging_steps=int(np.clip(np.round(train_epoch_steps/10),1,1)),\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\"\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "    callbacks=[PerplexityLoggingCallback()],  # Add the custom callback\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8531a825-2070-4302-a9a4-f0edc2f96d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_completed_steps = trainer.get_completed_steps()\n",
    "test_steps = int(np.clip(np.round(initial_completed_steps*.1/.9,0),1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba8994b-bad2-4a93-bb3d-39729cdc2caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = valid_data_list\n",
    "\n",
    "combined_text = tokenizer.eos_token.join(dataset)\n",
    "\n",
    "if(sample):\n",
    "    combined_text = combined_text[0:s_size]\n",
    "else:\n",
    "    pass\n",
    "\n",
    "all_sequences = get_sequences(combined_text, tokenizer, seq_length=seq_length)\n",
    "print(\"len(all_sequences)\",len(all_sequences))\n",
    "\n",
    "total_sequences = len(all_sequences)\n",
    "train_epoch_steps  = (total_sequences / (batch_size * gradient_accumulation_steps))*train_fraction\n",
    "test_epoch_steps  = (total_sequences / (batch_size * gradient_accumulation_steps))*(1-train_fraction)\n",
    "\n",
    "max_train_steps = int(train_epoch_steps * epochs)\n",
    "\n",
    "train_len = int(train_fraction * total_sequences) \n",
    "\n",
    "train_sequences = all_sequences[:train_len]\n",
    "test_sequences = all_sequences[train_len:]\n",
    "\n",
    "#train_dataset = CustomDataset(train_sequences)\n",
    "#test_dataset = CustomDataset(test_sequences)\n",
    "\n",
    "train_dataset = datasets.Dataset.from_dict({\"input_ids\": train_sequences})\n",
    "test_dataset = datasets.Dataset.from_dict({\"input_ids\": test_sequences})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52e3704-ff87-4773-8457-071b09419695",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_trainer = CustomTrainer(\n",
    "    model=trainer.model,\n",
    "    max_eval_steps=test_steps,  # Pass the test_steps here\n",
    "    train_dataset=test_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size = batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        max_steps=test_steps,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=int(np.clip(np.round(test_epoch_steps/10),1,1)),\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\"\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "test_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d98835-d689-4522-9f46-1b3bacd1e99c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b647ece0-7b47-4adb-87ea-7ee9a32f71ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_trainer.model.save_pretrained('./bitsft')\n",
    "test_trainer.model.config.use_cache = True\n",
    "generator = pipeline('text-generation', model=test_trainer.model, tokenizer = tokenizer)\n",
    "results = generator(\"To live well\", do_sample=True, min_length=50, max_length=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
