{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91de3ed-d5d1-4c62-b54a-f5af04e54bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, Trainer, EvalPrediction, TrainingArguments, TrainerControl, TrainerState, BitModel\n",
    "import math\n",
    "#from transformers.trainer_pt_utils import PredictionOutput\n",
    "from peft import LoraConfig, get_peft_model, LoraModel\n",
    "from peft import prepare_model_for_kbit_training, PeftModel, PeftConfig\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset#, Dataset\n",
    "import datasets\n",
    "import numpy as np\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "from typing import List, Optional  # Add the import statement at the beginning of your file\n",
    "from transformers import logging\n",
    "from typing import Dict, Optional, Any\n",
    "from tqdm import tqdm\n",
    "from transformers import TrainerState\n",
    "from datetime import datetime\n",
    "import copy\n",
    "from transformers import TrainerControl, TrainerState\n",
    "import tempfile\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import pickle\n",
    "from random import sample\n",
    "\"\"\"\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "from transformers import TrainerCallback\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.optim import AdamW\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517260a2-a489-4118-b7cc-f98b35fa8454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subset(dataset, num_examples):\n",
    "    indices = sample(range(len(dataset)), num_examples)\n",
    "    return dataset.select(indices)\n",
    "\n",
    "def filter_datasets_for_use_case(datasets, use_case):\n",
    "    filtered_datasets = {}\n",
    "    for key, value in datasets.items():\n",
    "        if value[use_case]:\n",
    "            filtered_datasets[key] = value[use_case]\n",
    "    return filtered_datasets\n",
    "\n",
    "def split_datasets(data_dict, ratio=0.7, random_state=None):\n",
    "    train_data = {}\n",
    "    test_data = {}\n",
    "    validation_indices = {}\n",
    "\n",
    "    for key, value in data_dict.items():\n",
    "        train, test, train_indices, test_indices = train_test_split(value, range(len(value)), train_size=ratio, random_state=random_state)\n",
    "        train_data[key] = train\n",
    "        test_data[key] = test\n",
    "        validation_indices[key] = test_indices\n",
    "\n",
    "    return train_data, test_data, validation_indices\n",
    "\n",
    "def unique_elements(lst):\n",
    "    result = []\n",
    "    seen = set()\n",
    "    for item in lst:\n",
    "        if item not in seen:\n",
    "            seen.add(item)\n",
    "            result.append(item)\n",
    "    return result\n",
    "\n",
    "class PerplexityLoggingCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args: TrainingArguments, state: TrainerState, control: TrainerControl,\n",
    "                    metrics: Dict[str, float], prefix=None, **kwargs):\n",
    "        if prefix is None:\n",
    "            prefix = \"eval\"\n",
    "        eval_loss_key = f\"{prefix}_loss\"\n",
    "        if eval_loss_key in metrics:\n",
    "            loss = metrics[eval_loss_key]\n",
    "            metrics[f\"{prefix}_perplexity\"] = math.exp(loss)\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tensor_list):\n",
    "        self.tensor_list = tensor_list\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tensor_list[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensor_list)\n",
    "        \n",
    "def get_sequences(text, tokenizer, seq_length=768, stride_ratio=0.5):\n",
    "    all_token_ids = tokenizer.encode(text)\n",
    "\n",
    "    #Generate sequences using sliding window approach\n",
    "    stride_length = int(seq_length * stride_ratio)\n",
    "    sequences = []\n",
    "    for i in range(0, len(all_token_ids) - seq_length +1, stride_length):\n",
    "        input_ids = all_token_ids[i:i+seq_length]\n",
    "        sequences.append(input_ids)\n",
    "    \n",
    "    #Truncate the last sequence if it less than seq_length\n",
    "    last_sequence = sequences[-1]\n",
    "    if len(last_sequence) < seq_length:\n",
    "        last_sequence = last_sequence + [tokenizer.pad_token_id] * (seq_length - len(last_sequence))\n",
    "        sequences[-1] = last_sequence\n",
    "\n",
    "    #Drop any remaining sequences that are less than seq_length\n",
    "    sequences = [sequence for sequence in sequences if len(sequence) == seq_length]\n",
    "\n",
    "    return sequences\n",
    "\n",
    "def evaluate(model, dataloader, device, max_eval_steps):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        # Extract input_ids and convert them to tensors\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device) if 'labels' in batch else None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            input_dict = {'input_ids': input_ids, 'labels': labels}\n",
    "            outputs = model(**input_dict)\n",
    "         \n",
    "        loss = outputs.loss.repeat(input_ids.shape[0])\n",
    "        losses.append(loss.detach())\n",
    "        if max_eval_steps > 0 and step >= max_eval_steps: break\n",
    "    loss = torch.mean(torch.cat(losses))\n",
    "    try:\n",
    "        perplexity = torch.exp(loss)\n",
    "    except OverflowError:\n",
    "        perplexity = torch.tensor(float(\"inf\"))\n",
    "    return loss.item(), perplexity.item()\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, max_eval_steps=0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.best_perplexity = float(\"inf\")\n",
    "        self.best_model_state_dict = None\n",
    "        self.no_improvement_counter = 0\n",
    "        self.passed_epoch_steps = False\n",
    "        self.max_eval_steps = max_eval_steps  # Add max_eval_steps as an attribute\n",
    "\n",
    "    def evaluation_loop(self, dataloader, description, prediction_loss_only=False, ignore_keys=None, metric_key_prefix='eval'):\n",
    "        eval_loss, perplexity = evaluate(self.model, dataloader, self.args.device, self.max_eval_steps)\n",
    "    \n",
    "        # Check if epoch_steps are surpassed\n",
    "        if self.state.epoch >= 1:\n",
    "            self.passed_epoch_steps = True\n",
    "    \n",
    "        # Check for improvements if the epoch_steps are surpassed\n",
    "        if self.passed_epoch_steps:\n",
    "            if perplexity < self.best_perplexity:\n",
    "                self.best_perplexity = perplexity\n",
    "                self.best_model_state_dict = {k: v.clone().to('cpu') for k, v in self.model.state_dict().items()}\n",
    "                self.no_improvement_counter = 0\n",
    "            else:\n",
    "                self.no_improvement_counter += 1\n",
    "    \n",
    "        # Stop training, load the best state_dict in the model, and return the best_model if the perplexity did not improve 3 times consecutively\n",
    "        if self.no_improvement_counter == 3:\n",
    "            if self.best_model_state_dict:\n",
    "                self.model.load_state_dict(self.best_model_state_dict)\n",
    "            self.model.to(self.args.device)\n",
    "            self.control.should_training_stop = True\n",
    "            print(\"Training stopped, best model loaded with Perplexity:\", self.best_perplexity)\n",
    "    \n",
    "        self.log({\n",
    "            \"eval_loss\": eval_loss,\n",
    "            \"perplexity\": perplexity,\n",
    "            \"epoch\": self.state.epoch,\n",
    "        })\n",
    "    \n",
    "        # Define num_samples as the total number of samples in the dataloader\n",
    "        #num_samples = len(dataloader.dataset)\n",
    "    \n",
    "        # Initialize an instance of EvalPrediction without the 'metrics' keyword argument \n",
    "        #eval_prediction = EvalPrediction(predictions=None, label_ids=None, num_samples=num_samples)\n",
    "        eval_prediction = EvalPrediction(predictions=None, label_ids=None)\n",
    "        \n",
    "        # Define num_samples as the total number of samples in the dataloader\n",
    "        num_samples = len(dataloader.dataset)\n",
    "    \n",
    "        # Add the num_samples attribute to the eval_prediction instance\n",
    "        eval_prediction.num_samples = num_samples\n",
    "    \n",
    "        # Set the metrics dictionary\n",
    "        eval_prediction.metrics = {\"eval_loss\": eval_loss}\n",
    "    \n",
    "        return eval_prediction\n",
    "    \n",
    "    def get_completed_steps(self):\n",
    "        return self.state.global_step\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ed5da9-cff8-4ae8-b27f-16df0a16cb81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5966da-98d0-4321-bff0-923509d368cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#sample # of character's from combined_text\n",
    "sample=False\n",
    "#if true, what # of characters to sample (this * avg_tokens_per_char = rough # of tokens)\n",
    "#(10000*.6)/2/128*.1\n",
    "s_size = 10000\n",
    "seq_length = 128\n",
    "#seq_length = 128\n",
    "batch_size = 16\n",
    "epoch_steps_warmup_ratio = 1/3\n",
    "epochs = 10\n",
    "model_id = \"EleutherAI/gpt-neo-1.3B\"\n",
    "#model_id = \"EleutherAI/gpt-neo-125M\"\n",
    "warm_ratio = 1/2\n",
    "train_fraction = 0.9\n",
    "epochs = 3\n",
    "gradient_accumulation_steps = 16\n",
    "seed = 42\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    #bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_quant_type=\"fp4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8, \n",
    "    lora_alpha=32, \n",
    "    #target_modules=[\"query_key_value\"], \n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "infer_peft_config = PeftConfig.from_pretrained('bitsft')\n",
    "#BitModel.from_pretrained(peft_config)\n",
    "infer_model = AutoModelForCausalLM.from_pretrained(\n",
    "        infer_peft_config.base_model_name_or_path,\n",
    "        quantization_config=bnb_config, device_map={\"\":0}\n",
    ")\n",
    "\n",
    "\n",
    "#infer_model.gradient_checkpointing_enable()\n",
    "#infer_model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "#infer_model = get_peft_model(infer_model, lora_config)\n",
    "\n",
    "#print_trainable_parameters(infer_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a63ffa0-9ff9-4813-8951-9d08fbc44a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"EleutherAI/gpt-neo-1.3B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "new_tokens = {'additional_special_tokens': ['<|Context|>', '<|Prompt|>', '<|Response|>']}\n",
    "tokenizer.add_special_tokens(new_tokens)\n",
    "tokenizer.pad_token = '[PAD]'\n",
    "infer_model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b886fe30-ffbe-4c7d-b7ce-7b0feacdfbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_model.config.use_cache = True\n",
    "\n",
    "generator = pipeline('text-generation', model=infer_model, tokenizer = tokenizer)\n",
    "#results = generator(r\"Context:\\nPrompt: Finish the quote: 'To live well...'\\nResponse:\", do_sample=True, min_length=50, max_length=200)\n",
    "results = generator(r\"Context:\\nPrompt: If value A is 2, and value B is 3, what is A+B?'\\nResponse:\", do_sample=True, min_length=50, max_length=200)\n",
    "print(results[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17868161-7357-4322-87a1-5c8aa4210b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the meaning of life? Itâ€™s a question that will be asked as many people on the planet die every day, but no one really knows what it means.\n",
      "\n",
      "The questions that will be asked are about whether you will feel the pain of death, whether you will know if your loved one is dead, whether you will know if you were lucky or unlucky to be alive, and whether you will know how to cope with death. Death is hard to see, sometimes impossible to see, and it is a big reason to ask the bigger questions about your place in life and into the world.\n",
      "\n",
      "It is a question that can help us to understand our place in the world. The question also can help with questions about meaning. Meaning is a big question, but it is not one question. Meaning is many and you can ask many questions. The question is not about death, it is about what we can and must do to survive. It is about what we want to\n"
     ]
    }
   ],
   "source": [
    "infer_model.config.use_cache = True\n",
    "\n",
    "generator = pipeline('text-generation', model=infer_model, tokenizer = tokenizer)\n",
    "#results = generator(r\"Context:\\nPrompt: Finish the quote: 'To live well...'\\nResponse:\", do_sample=True, min_length=50, max_length=200)\n",
    "results = generator(r\"What is the meaning of life?\", do_sample=True, min_length=50, max_length=200)\n",
    "print(results[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5bedc9-874b-449b-9813-246684e77b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference wrapper function\n",
    "def generate_response(input_text: str):\n",
    "    input_text_tokens = (\n",
    "        input_text.replace(\"Context: \", \"<|Context|>\")\n",
    "        .replace(\"Prompt: \", \"<|Prompt|>\")\n",
    "    )\n",
    "    input_ids = tokenizer.encode(input_text_tokens + \" <|Response|>\", return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids, max_length=200, num_return_sequences=1)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Use the wrapper function for user-friendly input\n",
    "user_input = \"Context: Your context here Prompt: Your prompt here\"\n",
    "generated_response = generate_response(user_input)\n",
    "print(generated_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
